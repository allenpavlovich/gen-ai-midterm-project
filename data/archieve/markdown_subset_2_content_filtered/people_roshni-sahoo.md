---
title: Roshni Sahoo â€“ DSI
original_url: https://datascience.uchicago.edu/people/roshni-sahoo
category: people
date: 2025-05-04
---

<!-- Table-like structure detected -->

!

## Share

* Email page on Facebook (opens new window)
* Share page on X (opens new window)
* Email Page (opens new window)

<!-- Table-like structure detected -->

People / Non-DSI

# Roshni Sahoo

PhD Student, Stanford University

**Bio:**Roshni Sahoo is a PhD student in Computer Science at Stanford University, advised by Stefan Wager. She studies data-driven decision making, with a focus on applications in public health and medicine. Her work contributes statistical methodologies for tackling challenges such as biased sample selection, heavy-tailed data, and uncertainty quantification. Her research is supported by a Spectrum Population Health Sciences Pilot Grant, NSF Graduate Research Fellowship, Stanford Data Science Scholar Award, and Stanford Ethics in Society Fellowship. Prior to Stanford, she received BS degrees in computer science and mathematics at MIT.

**Talk Title:**Learning from a Biased Sample

**Abstract:** The empirical risk minimization approach to data-driven decision making assumes that we can learn a decision rule from training data drawn under the same conditions as the ones we want to deploy it in. However, in a number of settings, we may be concerned that our training sample is biased, and that some groups (characterized by either observable or unobservable attributes) may be under- or over-represented relative to the general population; and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment. We propose a model of sampling bias called Gamma-biased sampling, where observed covariates can affect the probability of sample selection arbitrarily much but the amount of unexplained variation in the probability of sample selection is bounded by a constant factor. Applying the distributionally robust optimization framework, we propose a method for learning a decision rule that minimizes the worst-case risk incurred under a family of test distributions that can generate the training distribution under Gamma-biased sampling. We apply a result of Rockafellar and Uryasev to show that this problem is equivalent to an augmented convex risk minimization problem. We give statistical guarantees for learning a model that is robust to sampling bias via the method of sieves, and propose a deep learning algorithm whose loss function captures our robust learning target. We empirically validate our proposed method in simulations and a case study on ICU length of stay prediction.

Contact Info

Website

<https://roshni714.github.io/>