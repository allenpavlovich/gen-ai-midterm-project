---
title: Maryam Fazel (University of Washington) – Flat Minima and Generalization in Learning: the Case of Low-rank Matrix Recovery – DSI
original_url: https://datascience.uchicago.edu/events/maryam-fazel-washington-flat-minima
category: events
date: 2025-05-04
---

Oct

6

2023

## Share

* Email page on Facebook (opens new window)
* Share page on X (opens new window)
* Email Page (opens new window)

<!-- Table-like structure detected -->

Oct

6

2023

Past EventOct 06, 2023

# Maryam Fazel (University of Washington) – Flat Minima and Generalization in Learning: the Case of Low-rank Matrix Recovery

October 6, 2023 12:00 PM – 1:30 PM

John Crerar Library, Room 390

*Part of the [Autumn 2023 Distinguished Speaker Series](https://datascience.uchicago.edu/news/autumn-2023-distinguished-speaker-series/).*

![A headshot of Maryam Fazel is accompanied by text describing the Distinguished Speaker Series event: it takes place at 12:30pm in John Crerar Library Room 390.](http://datascience.uchicago.edu/wp-content/uploads/2023/09/Maryam-Fazel-DSS-2023-Header-2-2-scaled.jpg)

[Image: A headshot of Maryam Fazel is accompanied by text describing the Distinguished Speaker Series event: it takes place at 12:30pm in John Crerar Library Room 390.]

Many behaviors empirically observed in deep neural networks still lack satisfactory explanation; e.g., how does an overparameterized neural network avoid overfitting and generalize to unseen data?  Empirical evidence suggests that generalization depends on *which* zero-loss local minimum is attained during training. The shape of the training loss around a local minimum seems to strongly impact the model’s performance: “Flat” local minima—around which the loss grows slowly—appear to generalize well. Clarifying this phenomenon can help explain generalization properties, which still largely remain a mystery.

Towards this goal, in this talk we focus on the simplest class of overparameterized nonlinear models, those arising in low-rank matrix recovery. We study the following key models: overparametrized matrix sensing, bilinear sensing and phase retrieval, robust Principal Component Analysis, covariance matrix estimation, and single hidden layer neural networks with quadratic activation. We prove that in all these models, flat minima (measured by the trace of the Hessian, a notion of average curvature) exactly recover the ground truth, under standard statistical assumptions. These results suggest (i) a theoretical basis for favoring methods that bias iterates towards flat solutions, (ii) use of Hessian trace as a good regularizer for some learning tasks. Since the landscape properties we proved are algorithm-agnostic, a future direction is to pair these findings with the analysis of common training algorithms to understand the interplay between the loss landscape and algorithmic implicit bias.

**Bio**: [Maryam Fazel](https://people.ece.uw.edu/fazel_maryam/) is the Moorthy Family Professor of Electrical and Computer Engineering at the University of Washington, with adjunct appointments in Computer Science and Engineering, Mathematics, and Statistics. Maryam received her MS and PhD from Stanford University, her BS from Sharif University of Technology in Iran, and was a postdoctoral scholar at Caltech before joining UW. She is a recipient of the NSF Career Award, UWEE Outstanding Teaching Award, a UAI conference Best Student Paper Award with her student. She directs the Institute for Foundations of Data Science (IFDS), a multi-site NSF TRIPODS Institute. She serves on the Editorial board of the MOS-SIAM Book Series on Optimization, and is an Associate Editor of the SIAM Journal on Mathematics of Data Science. Her current research interests are in the area of optimization in machine learning and control.

## Agenda

### Friday, October 6, 2023

12:00pm–12:30pm

#### Lunch

Lunch will be provided on a first come, first serve basis.

12:30pm–1:30pm

#### Talk and Q&A

Related

News

* [DSI NewsSep 07, 2023

  ## Autumn 2023 Distinguished Speaker Series](https://datascience.uchicago.edu/news/autumn-2023-distinguished-speaker-series/)

Events

* [Past EventNov 03, 2023

  ## Elizabeth Barnes (Colorado State University) – Explainable AI for Climate Science: Opening the black box to reveal planet Earth](https://datascience.uchicago.edu/events/elizabeth-barnes-colorado-state-university-explainable-ai/)
* [Past EventOct 13, 2023

  ## Adam Klivans (UT Austin) – Testable Learning](https://datascience.uchicago.edu/events/adam-klivans-ut-austin-testable-learning/)
* [Past EventNov 10, 2023

  ## Milind Tambe (Harvard) – Integrating ML+Optimization: Driving Social Impact in public health and conservation](https://datascience.uchicago.edu/events/milind-tambe-harvard-ai-for-social-impact/)
* [Past EventNov 17, 2023

  ## Q. Vera Liao (Microsoft Research) – Human-Centered AI Transparency](https://datascience.uchicago.edu/events/vera-liao-microsoft-research-human-centered-ai-transparency/)

+ Show All
- Show Less

## More on this topic

[!

May

2

Past EventMay 02, 2025

## UnCommon Core | AI and the Future of Work with Career Advancement](https://datascience.uchicago.edu/events/uncommon-core-ai-and-the-future-of-work-with-career-advancement/)
[!

Nov

4

Upcoming EventNov 04, 2024

## Esteban Real (Google DeepMind): Automatically Discovering Learning Algorithms with Hardware Constraints](https://datascience.uchicago.edu/events/esteban-real-automatically-discovering-learning-algorithms-with-hardware-constraints/)
[!

Nov

8

Past EventNov 08, 2024

## Max Nickel (FAIR, Meta AI) – Towards Social Foundations of AI](https://datascience.uchicago.edu/events/max-nickel-fair-meta-ai-towards-social-foundations-of-ai/)
[!

Nov

21

Past EventNov 21, 2024

## NSF Workshop on Data-driven Modeling and Prediction of Rare and Extreme Events](https://datascience.uchicago.edu/events/nsf-workshop-on-data-driven-modeling-and-prediction-of-rare-and-extreme-events/)