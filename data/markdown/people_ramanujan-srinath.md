---
title: Ramanujan Srinath – DSI
original_url: https://datascience.uchicago.edu/people/ramanujan-srinath
category: people
date: 2025-05-04
---

<!-- Table-like structure detected -->

!

## Share

* Email page on Facebook (opens new window)
* Share page on X (opens new window)
* Email Page (opens new window)

<!-- Table-like structure detected -->

People / Post-Doctoral Scholars

# Ramanujan Srinath

Eric and Wendy Schmidt AI in Science Postdoctoral Fellow

**RESEARCH**:

To understand the neural computations that lead to object understanding and guide flexible, naturalistic behaviors, Ramanujan Srinath deploys AI techniques to inform efficient visual neuroscientific experiments. Using closed-loop neuroscientific experiments (AI generates hypotheses -> experimental data informs AI models) Srinath will test the central hypothesis that learned associations between object-scene properties affect the inference of those properties to guide behaviour.

**BIO**:

Ramanujan Srinath’s research focuses on understanding how the brain processes visual information to guide flexible behavior. Srinath uses electrophysiological, psychophysical, and computational techniques to study how the primate visual system processes objects (presented on a screen) and how inferences about those objects are mapped to behavioural outputs in different environmental, cognitive, and task conditions. During Srinath’s Ph.D. in the labs of Drs. Ed Connor and Kristina Nielsen, Srinath studied how 3D object information is extracted from 2D images using single-unit extracellular electrophysiology and two-photon imaging in monkeys. Srinath brought experience with algorithms to generate parameterised, naturalistic 3D visual stimuli to their postdoc in the lab of Dr. Marlene Cohen. The broad goal of Srinath’s research programme is to understand how the visual brain enables people to interact flexibly with the world by inferring relevant properties of 3D objects in naturalistic environments.

[Website](https://www.ramsrinath.com/)

## Related News, Insights, and Past Events

<!-- Table-like structure detected -->

[![AI methods and models have enabled a huge leap in our understanding of how images are processed in the brain.
We used to describe visual neurons as “edge detectors” and “face detectors”. Using deep neural networks, we have
discovered that images like these (which really can’t be described with words) are richer models of single neurons in
our visual system. I liken these AI-enabled descriptions of neural function, perhaps ironically, to a whole new kind of
vocabulary that neuroscientists can now use to explain the visual system. (Images from various papers including a,b, c, d, e , f)](https://datascience.uchicago.edu/wp-content/uploads/2024/04/f1-750x500.png)

[Image: AI methods and models have enabled a huge leap in our understanding of how images are processed in the brain.
We used to describe visual neurons as “edge detectors” and “face detectors”. Using deep neural networks, we have
discovered that images like these (which really can’t be described with words) are richer models of single neurons in
our visual system. I liken these AI-enabled descriptions of neural function, perhaps ironically, to a whole new kind of
vocabulary that neuroscientists can now use to explain the visual system. (Images from various papers including a,b, c, d, e , f)]

BlogMay 16, 2024

## Expanding Our Vocabulary of Vision Using AI](https://datascience.uchicago.edu/insights/expanding-our-vocabulary-of-vision-using-ai/)