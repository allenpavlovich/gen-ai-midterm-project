---
title: Blair Bilodeau – DSI
original_url: https://datascience.uchicago.edu/people/blair-bilodeau
category: people
date: 2025-05-04
---

<!-- Table-like structure detected -->

!

## Share

* Email page on Facebook (opens new window)
* Share page on X (opens new window)
* Email Page (opens new window)

<!-- Table-like structure detected -->

People / Non-DSI

# Blair Bilodeau

PhD Candidate, University of Toronto

**Bio:** Blair Bilodeau is a graduating PhD candidate in statistical sciences at the University of Toronto, advised by Daniel Roy. His PhD was funded by an NSERC Doctoral Canada Graduate Scholarship and the Vector Institute. Blair’s research area is broadly the theoretical foundations of statistical decision making, with applications to both statistical methodology and machine learning algorithms. His contributions span fundamental problems in inference and prediction, including: adaptive sequential decision making, minimax conditional density estimation, and numerical quadrature for statistical inference. Previously, Blair completed his BSc at Western University.

**Talk Title:** Adaptively Exploiting d-Separators with Causal Bandits

**Talk Abstract:** Multi-armed bandit problems provide a framework to identify the optimal intervention over a sequence of repeated experiments. Without additional assumptions, minimax optimal performance (measured by cumulative regret) is well-understood. With access to additional observed variables that d-separate the intervention from the outcome (i.e., they are a d- separator), recent “causal bandit” algorithms provably incur less regret. However, in practice it is desirable to be agnostic to whether observed variables are a d-separator. Ideally, an algorithm should be adaptive; that is, perform nearly as well as an algorithm with oracle knowledge of the presence or absence of a d-separator. In this work, we formalize and study this notion of adaptivity, and provide a novel algorithm that simultaneously achieves (a) optimal regret when a d-separator is observed, improving on classical minimax algorithms, and (b) significantly smaller regret than recent causal bandit algorithms when the observed variables are not a d-separator. Crucially, our algorithm does not require any oracle knowledge of whether a d-separator is observed. We also generalize this adaptivity to other conditions, such as the front-door criterion.