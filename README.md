# ğŸ“ UChicago MS in Applied Data Science â€“ RAG Chatbot

# UChicago MS in Applied Data Science - RAG Chatbot Project

This project implements a **Retrieval-Augmented Generation (RAG)** chatbot designed to answer questions about the University of Chicago's MS in Applied Data Science program by leveraging web content, vector embeddings, and LLM integration.

---

## ğŸš€ Project Overview

- Extracts comprehensive information from the entire [UChicago Data Science Institute website](https://datascience.uchicago.edu/)
- Uses a two-phase approach: link discovery followed by content processing
- Converts webpage data into structured Markdown files with metadata
- Will generate semantic embeddings for efficient retrieval
- Aims to provide an interactive conversational QA interface

---

## ğŸ“ Project Structure

```plaintext
gen-ai-midterm-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ discovered_links.json   # Links discovered during crawling phase
â”‚   â”œâ”€â”€ processed_links.json    # Status tracking of content processing
â”‚   â”œâ”€â”€ markdown/               # Converted Markdown files
â”‚   â”œâ”€â”€ category_summary.csv    # Summary of content by category
â”‚   â””â”€â”€ word_count_summary.csv  # Analysis of content length
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ .gitkeep                # Directory for logging output
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ link_discovery.ipynb    # Phase 1: Web crawling to identify content
â”‚   â””â”€â”€ content_processor.ipynb # Phase 2: Content extraction and conversion
â”œâ”€â”€ requirements.txt            # Project dependencies
â”œâ”€â”€ .gitignore                  # Git exclusion rules
â””â”€â”€ README.md                   # Project documentation
```

## ğŸ“Š Project Phases

### Phase 1: Link Discovery âœ…
- Comprehensive crawling of the datascience.uchicago.edu domain
- Intelligent filtering of relevant content
- Storage of discovered links for subsequent processing

### Phase 2: Content Processing âœ…
- Extraction of content from discovered links
- Conversion to structured Markdown format
- Metadata addition (title, URL, category, date)
- Basic content analysis (word count, category distribution)

### Phase 3: Data Cleanup ğŸ”„
- Removal of boilerplate content (navigation, share buttons)
- Text normalization
- Handling of special characters and formatting
- Deduplication of similar content

### Phase 4: Vector Embedding (Pending)
- Chunking strategy implementation
- Embedding generation
- Integration with vector database

### Phase 5: RAG Implementation (Pending)
- Retrieval component development using vector similarity search
- Agent-based implementation with **LangGraph** for complex reasoning
- Generation component integration with LLM
- Conversational interface with memory and state tracking

---

## ğŸ› ï¸ Setup and Usage

### Prerequisites
- Python 3.8+ installed on your system
- Git installed on your system
- Jupyter environment (Jupyter Lab or Notebook)

### Installation

**Windows:**
```powershell
# Clone the repository
git clone <repository-url>
cd gen-ai-midterm-project

# Create and activate virtual environment (optional but recommended)
python -m venv venv
.\venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

**macOS/Linux:**
```bash
# Clone the repository
git clone <repository-url>
cd gen-ai-midterm-project

# Create and activate virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Running the Project (already done, no need to repeat)

1. Launch Jupyter Lab/Notebook:
   ```
   jupyter lab
   # or
   jupyter notebook
   ```

2. Run notebooks in sequence:
   - First run `notebooks/link_discovery.ipynb` to collect all website links
   - Then run `notebooks/content_processor.ipynb` to extract and process content

### Project Output

After running the notebooks, you'll find:
- Processed markdown files in the `data/markdown/` directory
- Analysis files in the `data/` directory
- Log files in the `logs/` directory

---
