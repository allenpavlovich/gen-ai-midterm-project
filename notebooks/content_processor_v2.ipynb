{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UChicago MS in Applied Data Science Website - Content Processor\n",
    "\n",
    "This notebook handles Phase 2 of the scraping process: processing the previously discovered links and converting their content to Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from markdownify import markdownify as md\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path # For more robust path handling\n",
    "import unicodedata # For slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import trafilatura\n",
    "    TRAFILATURA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRAFILATURA_AVAILABLE = False\n",
    "    print(\"Trafilatura library not found. Falling back to BeautifulSoup for content extraction. \"\n",
    "          \"For potentially better results, consider installing it (`pip install trafilatura`).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://datascience.uchicago.edu/\"\n",
    "# Directory where data files (JSON, CSV) are stored/will be stored\n",
    "DATA_DIR = \"../data\"\n",
    "# Directory where Markdown files will be saved\n",
    "MARKDOWN_DIR = os.path.join(DATA_DIR, \"markdown_processed\")\n",
    "# Directory for log files\n",
    "LOGS_DIR = \"../logs\"\n",
    "# JSON file containing discovered links (output from link_discovery.ipynb)\n",
    "LINKS_FILE = os.path.join(DATA_DIR, \"discovered_links.json\")\n",
    "# JSON file to store processing status\n",
    "PROCESSED_FILE = os.path.join(DATA_DIR, \"processed_links_status.json\")\n",
    "# Delay between requests (in seconds)\n",
    "REQUEST_DELAY = 0.15\n",
    "# Timeout for requests (in seconds)\n",
    "REQUEST_TIMEOUT = 20 # Increased timeout for content fetching\n",
    "# User-Agent for requests\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 GenAICodeReviewBot/1.0 ContentProcessor/1.0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(MARKDOWN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOGS_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = Path(LOGS_DIR) / f\"content_processor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(funcName)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding='utf-8'),\n",
    "        logging.StreamHandler()  # Also log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({'User-Agent': USER_AGENT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Convert to a slug.\n",
    "    Converts to lowercase, removes non-word characters (alphanumerics and\n",
    "    underscores) and converts spaces to hyphens. Also strips leading and\n",
    "    trailing whitespace.\n",
    "    Adapted from Django's slugify function.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize('NFKC', value)\n",
    "    else:\n",
    "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
    "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
    "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_discovered_links(filepath):\n",
    "    \"\"\"\n",
    "    Load the previously discovered links from the JSON file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        logger.error(f\"Links file not found: {filepath}\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        links = data.get('links', [])\n",
    "        discovery_date = data.get('discovery_date', 'Unknown')\n",
    "        logger.info(f\"Loaded {len(links)} links, discovered on {discovery_date}, from {filepath}\")\n",
    "        return links\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON from {filepath}: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error loading links from {filepath}: {e}\", exc_info=True)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_html(url):\n",
    "    \"\"\"\n",
    "    Fetch the HTML content of a page.\n",
    "    Returns HTML string or None on error.\n",
    "    \"\"\"\n",
    "    if url.startswith('mailto:') or url.startswith('tel:'):\n",
    "        logger.info(f\"Skipping non-HTTP(S) URL: {url}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        response = session.get(url, timeout=REQUEST_TIMEOUT)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4XX or 5XX)\n",
    "        \n",
    "        content_type = response.headers.get('Content-Type', '').lower()\n",
    "        if 'text/html' not in content_type:\n",
    "            logger.warning(f\"Content-Type for {url} is not text/html ({content_type}). Skipping HTML processing.\")\n",
    "            return None # Or handle differently, e.g., save raw content if it's text-based\n",
    "            \n",
    "        # Try to decode using apparent encoding, then fall back to utf-8\n",
    "        try:\n",
    "            html_content = response.content.decode(response.apparent_encoding)\n",
    "        except (UnicodeDecodeError, TypeError):\n",
    "            logger.warning(f\"Failed to decode with apparent_encoding for {url}, falling back to utf-8.\")\n",
    "            html_content = response.content.decode('utf-8', errors='replace')\n",
    "            \n",
    "        return html_content\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout while fetching {url}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(f\"HTTP error fetching {url}: {e.response.status_code} {e.response.reason}\")\n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        logger.error(f\"Connection error fetching {url}: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Generic request error fetching {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error fetching HTML for {url}: {e}\", exc_info=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_from_soup(soup):\n",
    "    \"\"\"\n",
    "    Extract the title from a BeautifulSoup object.\n",
    "    Prioritizes <title>, then <meta property=\"og:title\">, then <h1>.\n",
    "    \"\"\"\n",
    "    if soup.title and soup.title.string:\n",
    "        return soup.title.string.strip()\n",
    "    \n",
    "    og_title = soup.find('meta', property='og:title')\n",
    "    if og_title and og_title.get('content'):\n",
    "        return og_title['content'].strip()\n",
    "        \n",
    "    h1 = soup.find('h1')\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        return h1.get_text(strip=True)\n",
    "        \n",
    "    return \"No Title Found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_content_with_trafilatura(html_content, url):\n",
    "    \"\"\"\n",
    "    Extract main content using Trafilatura.\n",
    "    Returns extracted text or None.\n",
    "    \"\"\"\n",
    "    if not TRAFILATURA_AVAILABLE:\n",
    "        return None\n",
    "    try:\n",
    "        # include_links=True can be useful for RAG context\n",
    "        # favor_precision=True might give cleaner but potentially shorter output\n",
    "        extracted_text = trafilatura.extract(html_content, \n",
    "                                             url=url,\n",
    "                                             include_comments=False, \n",
    "                                             include_tables=True, # Trafilatura can extract tables\n",
    "                                             favor_recall=True) # Or favor_precision=True\n",
    "        if extracted_text:\n",
    "            logger.info(f\"Successfully extracted content using Trafilatura for {url}\")\n",
    "            return extracted_text # Trafilatura returns text, not HTML soup for markdownify\n",
    "        else:\n",
    "            logger.warning(f\"Trafilatura returned no content for {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error using Trafilatura for {url}: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_content_with_bs(soup, url):\n",
    "    \"\"\"\n",
    "    Extract the main content from a BeautifulSoup object using heuristics.\n",
    "    This is a fallback if Trafilatura is not available or fails.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Attempting main content extraction with BeautifulSoup for {url}\")\n",
    "    # Common semantic tags for main content\n",
    "    content_selectors = ['main', 'article', 'div.content', 'div.entry-content', 'div#content', \n",
    "                         'div.main-content', 'div.td-post-content'] # Added more specific selectors\n",
    "    \n",
    "    for selector in content_selectors:\n",
    "        element = soup.select_one(selector)\n",
    "        if element:\n",
    "            logger.info(f\"Found main content using selector '{selector}' for {url}\")\n",
    "            return str(element) # Return as HTML string for markdownify\n",
    "\n",
    "    logger.warning(f\"No specific main content element found for {url} using selectors. Falling back to soup.body.\")\n",
    "    return str(soup.body) if soup.body else str(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category_from_url(url):\n",
    "    \"\"\"\n",
    "    Extract a category from the URL path for better organization.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip('/')\n",
    "    \n",
    "    if not path or path == \"index.html\" or path == \"index.php\": # Handle homepage variations\n",
    "        return 'homepage'\n",
    "    \n",
    "    parts = path.split('/')\n",
    "    # Filter out empty parts that can result from multiple slashes, e.g. /news//article\n",
    "    parts = [part for part in parts if part] \n",
    "    \n",
    "    if len(parts) > 0:\n",
    "        # Take the first significant part as category, could be extended\n",
    "        # e.g. if parts[0] is 'people' and len(parts) > 1, category could be 'people/sub-category'\n",
    "        return parts[0] \n",
    "    \n",
    "    return 'uncategorized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_soup_for_markdown(soup, base_url):\n",
    "    \"\"\"\n",
    "    Modify the BeautifulSoup object in-place to improve Markdown conversion.\n",
    "    - Converts images to Markdown format with alt text and absolute URLs.\n",
    "    - (Future: Could add more enhancements here, e.g., for figures, blockquotes)\n",
    "    \"\"\"\n",
    "    # Process images: make src absolute and ensure alt text is present\n",
    "    for img in soup.find_all('img'):\n",
    "        alt_text = img.get('alt', '').strip()\n",
    "        if not alt_text: # If alt text is missing, try to use filename from src\n",
    "            src = img.get('src', '')\n",
    "            if src:\n",
    "                alt_text = os.path.basename(urlparse(src).path)\n",
    "                alt_text = alt_text.replace('-', ' ').replace('_', ' ').rsplit('.',1)[0] # Basic cleanup\n",
    "                alt_text = f\"Image: {alt_text.capitalize()}\" if alt_text else \"Image\" # Default if still empty\n",
    "        else:\n",
    "            alt_text = f\"Image: {alt_text}\"\n",
    "\n",
    "        src = img.get('src')\n",
    "        if src:\n",
    "            img['src'] = urljoin(base_url, src) # Ensure absolute URL\n",
    "        \n",
    "        # Replace img tag with a paragraph containing Markdown image syntax\n",
    "        # This helps markdownify handle it better or allows direct text representation\n",
    "        # markdown_image_text = f\"![{alt_text}]({img.get('src', '')})\"\n",
    "        # p_tag = soup.new_tag('p')\n",
    "        # p_tag.string = markdown_image_text\n",
    "        # img.replace_with(p_tag) \n",
    "        # Self-correction: markdownify handles <img> tags well. We just need to ensure src is absolute.\n",
    "        # The original approach of adding a <p> tag with \"[Image: alt_text]\" after the image\n",
    "        # might be better if we want both the image (if markdown viewer supports it) AND the text.\n",
    "        # For pure text RAG, just the alt text is key.\n",
    "        # Let's stick to the original idea of adding a descriptive paragraph if alt text exists.\n",
    "        if alt_text and alt_text != \"Image\": # Only add if meaningful alt text\n",
    "            desc_text = f\"[{alt_text}]\"\n",
    "            if img.get('src'):\n",
    "                 desc_text += f\" (Source: {img['src']})\"\n",
    "            \n",
    "            # Create a text node or a simple tag to represent this.\n",
    "            # A simple NavigableString might be best to avoid complex tag structures.\n",
    "            img_desc_node = NavigableString(f\"\\n{desc_text}\\n\")\n",
    "            img.insert_after(img_desc_node)\n",
    "\n",
    "\n",
    "    # Table handling: markdownify generally handles basic tables.\n",
    "    # Custom table conversion can be very complex for varied HTML.\n",
    "    # The previous custom table logic was removed in favor of relying on markdownify's capabilities.\n",
    "    # If markdownify's table output is insufficient, a more robust custom solution or\n",
    "    # pre-processing tables into a simpler HTML structure might be needed.\n",
    "    # For div-based \"tables\", a general solution is extremely hard.\n",
    "    # It's better to identify patterns on the specific site and write targeted extractors if needed.\n",
    "    # For now, we'll let markdownify do its best with tables.\n",
    "\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_markdown_content(html_input, url, is_full_html_page=True):\n",
    "    \"\"\"\n",
    "    Convert HTML content to Markdown.\n",
    "    If `is_full_html_page` is True, it expects a full HTML page to parse with BeautifulSoup.\n",
    "    If False, it assumes `html_input` is already processed text (e.g., from Trafilatura).\n",
    "    \"\"\"\n",
    "    if is_full_html_page: # Input is raw HTML string from get_main_content_with_bs\n",
    "        soup = BeautifulSoup(html_input, 'html.parser')\n",
    "        # Enhance soup (e.g., image alt text, absolute URLs) before markdownify\n",
    "        soup = enhance_soup_for_markdown(soup, url)\n",
    "        html_to_convert = str(soup)\n",
    "    else: # Input is already extracted text (e.g., from Trafilatura)\n",
    "        html_to_convert = html_input # Trafilatura might return plain text or minimal HTML\n",
    "\n",
    "    # Markdownify options\n",
    "    # More aggressive stripping can be done here if main content extraction is not perfect.\n",
    "    # e.g., strip=['script', 'style', 'nav', 'aside', 'footer', 'header']\n",
    "    # However, if get_main_content is effective, this might not be necessary.\n",
    "    options = {\n",
    "        'strip': ['script', 'style'], # Basic stripping\n",
    "        'heading_style': 'atx',      # Use '#' for headings\n",
    "        'bullets': '-*+',            # Cycle through bullet styles for nested lists\n",
    "        'strong_em_symbol': 'asterisk', # Use * for bold/italic\n",
    "        'code_language_callback': lambda el: el.get('class')[0].replace(\"language-\", \"\") if el.get('class') else None,\n",
    "        # 'default_title': True, # If you want markdownify to try to add a title if none exists\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        markdown_text = md(html_to_convert, **options)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Markdownify failed for {url}: {e}\", exc_info=True)\n",
    "        return f\"Error during Markdown conversion: {e}\" # Return error message in content\n",
    "\n",
    "    # Basic Markdown cleanup\n",
    "    markdown_text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)  # Remove excess newlines\n",
    "    markdown_text = re.sub(r'\\[\\s*\\]\\(([^)]+)\\)', r'\\1', markdown_text)  # Remove empty links, keep URL if text was empty\n",
    "    markdown_text = re.sub(r'!\\[\\s*\\]\\(([^)]+)\\)', r'[Image: \\1](\\1)', markdown_text) # Ensure alt text for images if missing\n",
    "    markdown_text = markdown_text.strip()\n",
    "    \n",
    "    return markdown_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_markdown_file(title, markdown_content, url, category, base_dir):\n",
    "    \"\"\"\n",
    "    Save markdown content to a file with YAML frontmatter.\n",
    "    Uses a slugified version of the URL path for the filename.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    path_parts = [part for part in parsed_url.path.strip('/').split('/') if part and part not in (\"index.html\", \"index.php\")]\n",
    "    \n",
    "    if not path_parts: # Handle homepage or root URLs\n",
    "        filename_base = 'index'\n",
    "    else:\n",
    "        # Slugify each part and join, or just the last part\n",
    "        # Using the full path slugified is generally safer for uniqueness\n",
    "        filename_base = slugify('-'.join(path_parts))\n",
    "        if not filename_base: # if slugify results in empty string (e.g. path was just '/')\n",
    "            filename_base = 'index'\n",
    "            \n",
    "    filename = f\"{filename_base}.md\"\n",
    "    filepath = Path(base_dir) / category / filename # Organize by category subdirectory\n",
    "    \n",
    "    # Create category directory if it doesn't exist\n",
    "    (Path(base_dir) / category).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Add frontmatter with metadata\n",
    "    frontmatter = (\n",
    "        f\"---\\n\"\n",
    "        f\"title: \\\"{title.replace('\\\"', '\\\\\\\"')}\\\"\\n\" # Escape quotes in title\n",
    "        f\"original_url: {url}\\n\"\n",
    "        f\"category: {category}\\n\"\n",
    "        f\"processing_date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "        f\"---\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    full_content = frontmatter + markdown_content\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(full_content)\n",
    "        logger.info(f\"Successfully saved Markdown to: {filepath}\")\n",
    "        return str(filepath)\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Failed to save Markdown for {url} to {filepath}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error saving Markdown for {url} to {filepath}: {e}\", exc_info=True)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processing_summary(processed_details, failed_urls, summary_filepath):\n",
    "    \"\"\"\n",
    "    Save the processing summary (processed URLs with details, failed URLs) to a JSON file.\n",
    "    \"\"\"\n",
    "    summary_data = {\n",
    "        'summary_date': datetime.now().isoformat(),\n",
    "        'total_processed_successfully': len(processed_details),\n",
    "        'total_failed': len(failed_urls),\n",
    "        'processed_details': processed_details, # List of dicts {url: str, filepath: str, title: str, category: str}\n",
    "        'failed_urls': failed_urls # List of dicts {url: str, error: str}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(summary_filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Successfully saved processing summary to {summary_filepath}\")\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Failed to save processing summary to {summary_filepath}: {e}\")\n",
    "    return summary_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_links():\n",
    "    \"\"\"\n",
    "    Main function to process all discovered links.\n",
    "    \"\"\"\n",
    "    links_to_process = load_discovered_links(LINKS_FILE)\n",
    "    if not links_to_process:\n",
    "        logger.error(\"No links loaded to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    processed_details_list = []\n",
    "    failed_urls_list = []\n",
    "    total_links = len(links_to_process)\n",
    "    \n",
    "    logger.info(f\"--- Starting Content Processing for {total_links} links ---\")\n",
    "\n",
    "    for i, url in enumerate(links_to_process):\n",
    "        logger.info(f\"Processing {i+1}/{total_links}: {url}\")\n",
    "\n",
    "        # Skip SVG files directly based on URL extension, as they won't be HTML\n",
    "        if url.lower().endswith('.svg'):\n",
    "            logger.info(f\"Skipping SVG file based on URL extension: {url}\")\n",
    "            # Optionally, add to failed_urls_list with a specific reason or handle differently\n",
    "            # failed_urls_list.append({\"url\": url, \"error\": \"Skipped SVG file\"})\n",
    "            continue\n",
    "\n",
    "        html_content = get_page_html(url)\n",
    "        if not html_content:\n",
    "            logger.warning(f\"Failed to get HTML content for {url}. Adding to failed list.\")\n",
    "            failed_urls_list.append({\"url\": url, \"error\": \"Failed to fetch HTML content\"})\n",
    "            time.sleep(REQUEST_DELAY) # Still delay even on failure\n",
    "            continue\n",
    "        \n",
    "        page_title = \"Untitled\" # Default title\n",
    "        markdown_output = \"\"\n",
    "        category = \"uncategorized\"\n",
    "\n",
    "        try:\n",
    "            # Attempt extraction with Trafilatura first if available\n",
    "            main_content_text = None\n",
    "            if TRAFILATURA_AVAILABLE:\n",
    "                main_content_text = extract_main_content_with_trafilatura(html_content, url)\n",
    "\n",
    "            if main_content_text:\n",
    "                # If Trafilatura succeeds, it returns text/minimal HTML.\n",
    "                # We still need a title. Parse the original HTML for it.\n",
    "                temp_soup_for_title = BeautifulSoup(html_content, 'html.parser')\n",
    "                page_title = get_title_from_soup(temp_soup_for_title)\n",
    "                markdown_output = html_to_markdown_content(main_content_text, url, is_full_html_page=False)\n",
    "            else:\n",
    "                # Fallback to BeautifulSoup for content extraction and title\n",
    "                logger.info(f\"Trafilatura failed or unavailable for {url}. Using BeautifulSoup fallback.\")\n",
    "                soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                page_title = get_title_from_soup(soup)\n",
    "                main_content_html_bs = extract_main_content_with_bs(soup, url)\n",
    "                if main_content_html_bs:\n",
    "                    markdown_output = html_to_markdown_content(main_content_html_bs, url, is_full_html_page=True)\n",
    "                else:\n",
    "                    logger.error(f\"BeautifulSoup fallback also failed to extract main content for {url}\")\n",
    "                    markdown_output = \"Error: Could not extract main content.\"\n",
    "            \n",
    "            category = extract_category_from_url(url)\n",
    "            \n",
    "            # Save the markdown content\n",
    "            saved_filepath = save_markdown_file(page_title, markdown_output, url, category, MARKDOWN_DIR)\n",
    "            \n",
    "            if saved_filepath:\n",
    "                processed_details_list.append({\n",
    "                    \"url\": url, \n",
    "                    \"filepath\": saved_filepath, \n",
    "                    \"title\": page_title, \n",
    "                    \"category\": category\n",
    "                })\n",
    "            else:\n",
    "                failed_urls_list.append({\"url\": url, \"error\": \"Failed to save Markdown file\"})\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Critical error processing {url}: {e}\", exc_info=True)\n",
    "            failed_urls_list.append({\"url\": url, \"error\": str(e)})\n",
    "        \n",
    "        # Save progress periodically (e.g., every 20 pages)\n",
    "        if (i + 1) % 20 == 0:\n",
    "            logger.info(f\"Processed {i+1} links. Saving intermediate summary...\")\n",
    "            save_processing_summary(processed_details_list, failed_urls_list, PROCESSED_FILE)\n",
    "            \n",
    "        time.sleep(REQUEST_DELAY)\n",
    "    \n",
    "    # Save final processing summary\n",
    "    logger.info(\"Content processing loop finished. Saving final summary...\")\n",
    "    save_processing_summary(processed_details_list, failed_urls_list, PROCESSED_FILE)\n",
    "    \n",
    "    logger.info(f\"--- Content Processing Completed ---\")\n",
    "    logger.info(f\"Successfully processed: {len(processed_details_list)} pages\")\n",
    "    logger.info(f\"Failed to process: {len(failed_urls_list)} pages\")\n",
    "    \n",
    "    return processed_details_list, failed_urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_category_summary(processed_details, output_dir):\n",
    "    \"\"\"Generates a CSV summary of processed pages by category.\"\"\"\n",
    "    if not processed_details:\n",
    "        logger.info(\"No processed details to generate category summary.\")\n",
    "        return\n",
    "\n",
    "    categories = {}\n",
    "    for item in processed_details:\n",
    "        category = item.get('category', 'uncategorized')\n",
    "        categories[category] = categories.get(category, 0) + 1\n",
    "    \n",
    "    category_df = pd.DataFrame({\n",
    "        'Category': list(categories.keys()),\n",
    "        'Count': list(categories.values())\n",
    "    }).sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    try:\n",
    "        summary_path = Path(output_dir) / 'processed_category_summary.csv'\n",
    "        category_df.to_csv(summary_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Category summary saved to {summary_path}\")\n",
    "        print(\"\\nCategory Summary:\")\n",
    "        print(category_df.to_string())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save category summary: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_count_summary(processed_details, output_dir):\n",
    "    \"\"\"Generates a CSV summary with word counts for each processed Markdown file.\"\"\"\n",
    "    if not processed_details:\n",
    "        logger.info(\"No processed details to generate word count summary.\")\n",
    "        return\n",
    "\n",
    "    word_counts_data = []\n",
    "    for item in processed_details:\n",
    "        filepath = item.get('filepath')\n",
    "        if filepath and os.path.exists(filepath):\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                # Remove frontmatter before counting words\n",
    "                content_body = re.sub(r'^---\\s*[\\s\\S]*?---', '', content, flags=re.DOTALL).strip()\n",
    "                words = re.findall(r'\\b\\w+\\b', content_body.lower()) # Basic word count\n",
    "                word_count = len(words)\n",
    "                word_counts_data.append({\n",
    "                    'Filename': os.path.basename(filepath),\n",
    "                    'Title': item.get('title', 'N/A'),\n",
    "                    'URL': item.get('url', 'N/A'),\n",
    "                    'Category': item.get('category', 'uncategorized'),\n",
    "                    'Word Count': word_count\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error counting words for {filepath}: {e}\", exc_info=True)\n",
    "                word_counts_data.append({\n",
    "                    'Filename': os.path.basename(filepath),\n",
    "                    'Title': item.get('title', 'N/A'),\n",
    "                    'URL': item.get('url', 'N/A'),\n",
    "                    'Category': item.get('category', 'uncategorized'),\n",
    "                    'Word Count': -1 # Indicate error\n",
    "                })\n",
    "        else:\n",
    "            logger.warning(f\"Filepath not found for word count: {filepath}\")\n",
    "\n",
    "    word_count_df = pd.DataFrame(word_counts_data).sort_values('Word Count', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    try:\n",
    "        summary_path = Path(output_dir) / 'processed_word_count_summary.csv'\n",
    "        word_count_df.to_csv(summary_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Word count summary saved to {summary_path}\")\n",
    "        print(\"\\nTop 10 Pages by Word Count:\")\n",
    "        print(word_count_df.head(10).to_string())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save word count summary: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_start_time = time.time()\n",
    "\n",
    "# Process all links\n",
    "processed_items, failed_items = process_all_links()\n",
    "\n",
    "# Generate summaries if processing was successful for some items\n",
    "if processed_items:\n",
    "    generate_category_summary(processed_items, DATA_DIR)\n",
    "    generate_word_count_summary(processed_items, DATA_DIR)\n",
    "else:\n",
    "    logger.info(\"No items were processed successfully, skipping summary generation.\")\n",
    "\n",
    "processing_end_time = time.time()\n",
    "elapsed_time = processing_end_time - processing_start_time\n",
    "logger.info(f\"--- Total Content Processing Script finished in {elapsed_time:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragchat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
