{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown Chunking for RAG System\n",
    "\n",
    "This notebook implements text chunking specifically optimized for Markdown content in a RAG system. It processes markdown files from `data/markdown_clean_final` and creates coherent chunks that preserve the structure of the documents for optimal RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm # For progress bars in Jupyter\n",
    "import re\n",
    "import logging # Added for better logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Import LangChain's Markdown text splitter and Document object\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "from langchain_core.documents import Document # For creating LangChain Document objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Input directory for clean markdown files\n",
    "INPUT_MARKDOWN_DIR = \"../data/markdown_clean_final\" \n",
    "# Output directory for chunked JSON files\n",
    "OUTPUT_CHUNKS_DIR = \"../data/chunks\"\n",
    "# Output directory for log files\n",
    "LOGS_DIR = \"../logs\" \n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 500   # Characters per chunk\n",
    "CHUNK_OVERLAP = 150  # Overlap between chunks to maintain context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories if they don't exist\n",
    "Path(OUTPUT_CHUNKS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(LOGS_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "log_file = Path(LOGS_DIR) / f\"markdown_chunking_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(module)s - %(funcName)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_frontmatter(content):\n",
    "    \"\"\"\n",
    "    Parse markdown frontmatter without external YAML dependencies.\n",
    "    \n",
    "    Args:\n",
    "        content (str): Markdown content with YAML frontmatter\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (metadata_dict, content_without_frontmatter)\n",
    "    \"\"\"\n",
    "    if not content.startswith('---'):\n",
    "        logger.debug(\"No frontmatter found (doesn't start with '---').\")\n",
    "        return {}, content\n",
    "    \n",
    "    parts = content.split('---', 2)\n",
    "    if len(parts) < 3:\n",
    "        logger.debug(\"Invalid frontmatter structure (not enough '---' separators).\")\n",
    "        return {}, content # Not a valid frontmatter structure\n",
    "    \n",
    "    frontmatter_text = parts[1].strip()\n",
    "    content_text = parts[2].strip() # The rest of the document\n",
    "    \n",
    "    metadata = {}\n",
    "    for line in frontmatter_text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or ':' not in line:\n",
    "            continue\n",
    "        \n",
    "        key, value = line.split(':', 1)\n",
    "        key = key.strip()\n",
    "        value = value.strip()\n",
    "        \n",
    "        # Remove quotes if present (handles both single and double)\n",
    "        if (value.startswith('\"') and value.endswith('\"')) or \\\n",
    "           (value.startswith(\"'\") and value.endswith(\"'\")):\n",
    "            value = value[1:-1]\n",
    "            \n",
    "        metadata[key] = value\n",
    "    \n",
    "    logger.debug(f\"Parsed frontmatter: {metadata}\")\n",
    "    return metadata, content_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markdown_splitter(chunk_size=500, chunk_overlap=150):\n",
    "    \"\"\"\n",
    "    Create a MarkdownTextSplitter with the specified parameters.\n",
    "    \"\"\"\n",
    "    markdown_splitter = MarkdownTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    logger.info(f\"MarkdownTextSplitter created with chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
    "    return markdown_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_category(file_path):\n",
    "    \"\"\"\n",
    "    Extract category from file path based on filename pattern.\n",
    "    For example: people_*.md -> 'people', news_*.md -> 'news', etc.\n",
    "    \"\"\"\n",
    "    file_name = Path(file_path).name\n",
    "    parts = file_name.split('_')\n",
    "    if len(parts) > 0:\n",
    "        return parts[0]\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_markdown_file(file_path, markdown_splitter):\n",
    "    \"\"\"\n",
    "    Split a markdown file into chunks while preserving metadata and markdown structure.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str or Path): Path to the markdown file\n",
    "        markdown_splitter (MarkdownTextSplitter): Configured markdown splitter\n",
    "        \n",
    "    Returns:\n",
    "        list: List of LangChain Document objects with content and metadata\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    logger.info(f\"Processing file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            full_file_content = f.read()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not read file {file_path}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    metadata, text_content = parse_frontmatter(full_file_content)\n",
    "    \n",
    "    # Add source information to metadata\n",
    "    metadata['source_file'] = str(file_path.resolve()) # Store absolute path\n",
    "    metadata['filename'] = file_path.name\n",
    "    metadata['category'] = extract_file_category(file_path)\n",
    "    \n",
    "    # Attempt to extract the first main heading from the content body\n",
    "    main_heading_match = re.search(r'^\\s*#\\s+(.+?)(?:\\n|$)', text_content, re.MULTILINE)\n",
    "    if main_heading_match:\n",
    "        metadata['main_heading'] = main_heading_match.group(1).strip()\n",
    "        logger.debug(f\"Extracted main_heading: {metadata['main_heading']}\")\n",
    "    else:\n",
    "        logger.debug(\"No main heading found in content body.\")\n",
    "\n",
    "    try:\n",
    "        # Split the content into chunks using the MarkdownTextSplitter\n",
    "        chunks_text = markdown_splitter.split_text(text_content)\n",
    "        logger.info(f\"Split '{file_path.name}' into {len(chunks_text)} text chunks.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error splitting text for {file_path.name}: {e}\", exc_info=True)\n",
    "        return []\n",
    "        \n",
    "    processed_chunks = []\n",
    "    for i, chunk_content_str in enumerate(chunks_text):\n",
    "        chunk_metadata = metadata.copy() # Start with base metadata from frontmatter\n",
    "        chunk_metadata['chunk_id'] = f\"{file_path.stem}_{i}\" # More unique chunk ID\n",
    "        chunk_metadata['chunk_number'] = i\n",
    "        chunk_metadata['total_chunks_in_file'] = len(chunks_text)\n",
    "        \n",
    "        # Create a LangChain Document object\n",
    "        try:\n",
    "            doc = Document(page_content=chunk_content_str.strip(), metadata=chunk_metadata)\n",
    "            processed_chunks.append(doc)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating Document object for chunk {i} in {file_path.name}: {e}\", exc_info=True)\n",
    "            # Fallback to dictionary if Document creation fails\n",
    "            processed_chunks.append({\n",
    "                'page_content': chunk_content_str.strip(),\n",
    "                'metadata': chunk_metadata,\n",
    "                'error_creating_document': True\n",
    "            })\n",
    "\n",
    "    return processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_markdown_directory(input_dir_path_str, output_dir_path_str, \n",
    "                               chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Process all markdown files in a directory, chunk them, and save chunks.\n",
    "    Chunks are saved to individual JSON files (one per original Markdown file)\n",
    "    and also aggregated into a single 'all_chunks.json' file.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir_path_str)\n",
    "    output_path = Path(output_dir_path_str)\n",
    "    output_path.mkdir(parents=True, exist_ok=True) # Ensure output directory exists\n",
    "    \n",
    "    markdown_splitter = create_markdown_splitter(chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Recursively find all .md files in all subdirectories of input_path\n",
    "    markdown_files = list(input_path.rglob(\"*.md\"))\n",
    "    logger.info(f\"Found {len(markdown_files)} markdown files in '{input_path}' and its subdirectories.\")\n",
    "    \n",
    "    if not markdown_files:\n",
    "        logger.warning(f\"No markdown files found in {input_path}. Exiting.\")\n",
    "        return 0, 0\n",
    "\n",
    "    all_processed_chunks_list = [] # To store all chunks from all files\n",
    "    total_files_processed = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for md_file_path in tqdm(markdown_files, desc=\"Processing Markdown Files\"):\n",
    "        file_chunks_data = split_markdown_file(md_file_path, markdown_splitter)\n",
    "        \n",
    "        if file_chunks_data:\n",
    "            total_files_processed += 1\n",
    "            \n",
    "            # Convert Document objects to dictionaries for JSON serialization\n",
    "            file_chunks_serializable = []\n",
    "            for chunk_item in file_chunks_data:\n",
    "                if isinstance(chunk_item, Document):\n",
    "                    file_chunks_serializable.append({\n",
    "                        \"page_content\": chunk_item.page_content,\n",
    "                        \"metadata\": chunk_item.metadata\n",
    "                    })\n",
    "                else: # It's already a dict (e.g., fallback from Document creation error)\n",
    "                    file_chunks_serializable.append(chunk_item)\n",
    "            \n",
    "            all_processed_chunks_list.extend(file_chunks_serializable)\n",
    "            \n",
    "            # Save chunks for each individual file\n",
    "            # Create a subdirectory structure in output_chunks_dir mirroring input_markdown_dir\n",
    "            relative_path = md_file_path.relative_to(input_path)\n",
    "            individual_chunk_output_dir = output_path / relative_path.parent\n",
    "            individual_chunk_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            output_json_path = individual_chunk_output_dir / f\"{md_file_path.stem}_chunks.json\"\n",
    "            \n",
    "            try:\n",
    "                with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(file_chunks_serializable, f, indent=2, ensure_ascii=False)\n",
    "                logger.info(f\"Saved {len(file_chunks_serializable)} chunks for {md_file_path.name} to {output_json_path}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Could not save chunks for {md_file_path.name} to {output_json_path}: {e}\", exc_info=True)\n",
    "        else:\n",
    "            logger.warning(f\"No chunks generated for {md_file_path.name}.\")\n",
    "\n",
    "    total_chunks_created = len(all_processed_chunks_list)\n",
    "    logger.info(f\"Successfully processed {total_files_processed} files, creating a total of {total_chunks_created} chunks.\")\n",
    "    \n",
    "    # Save all chunks to a single aggregated file\n",
    "    if all_processed_chunks_list:\n",
    "        all_chunks_aggregated_file_path = output_path / \"all_chunks_aggregated.json\"\n",
    "        try:\n",
    "            with open(all_chunks_aggregated_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(all_processed_chunks_list, f, indent=2, ensure_ascii=False) # Use indent for readability\n",
    "            logger.info(f\"Saved all {total_chunks_created} aggregated chunks to {all_chunks_aggregated_file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not save aggregated chunks file: {e}\", exc_info=True)\n",
    "            \n",
    "    return total_files_processed, total_chunks_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunk_distribution(all_chunks_aggregated_file_path_str):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of chunks from the aggregated JSON file.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    all_chunks_file_path = Path(all_chunks_aggregated_file_path_str)\n",
    "    if not all_chunks_file_path.exists():\n",
    "        logger.error(f\"Aggregated chunks file not found: {all_chunks_file_path}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(all_chunks_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            chunks_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not load or parse aggregated chunks file {all_chunks_file_path}: {e}\")\n",
    "        return None\n",
    "        \n",
    "    if not chunks_data:\n",
    "        logger.info(\"No chunks found in the aggregated file for analysis.\")\n",
    "        return None\n",
    "\n",
    "    chunk_lengths = [len(chunk.get(\"page_content\", chunk.get(\"content\", \"\"))) for chunk in chunks_data]\n",
    "    categories = [chunk.get(\"metadata\", {}).get(\"category\", \"unknown\") for chunk in chunks_data]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"length\": chunk_lengths,\n",
    "        \"category\": categories\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"\\n--- Chunk Analysis ---\")\n",
    "    logger.info(f\"Total chunks analyzed: {len(chunks_data)}\")\n",
    "    logger.info(f\"Average chunk length: {df['length'].mean():.2f} characters\")\n",
    "    logger.info(f\"Min chunk length: {df['length'].min()} characters\")\n",
    "    logger.info(f\"Max chunk length: {df['length'].max()} characters\")\n",
    "    \n",
    "    category_counts = df[\"category\"].value_counts()\n",
    "    logger.info(\"\\nCategory distribution of chunks:\")\n",
    "    for category, count in category_counts.items():\n",
    "        logger.info(f\"  - {category}: {count}\")\n",
    "    \n",
    "    # Plot distribution of chunk lengths\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.hist(chunk_lengths, bins=50, edgecolor='black')\n",
    "    plt.xlabel(\"Chunk Length (characters)\")\n",
    "    plt.ylabel(\"Number of Chunks\")\n",
    "    plt.title(\"Distribution of Chunk Lengths\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot category distribution\n",
    "    if not category_counts.empty:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        category_counts.plot(kind=\"bar\", color='skyblue', edgecolor='black')\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Number of Chunks\")\n",
    "        plt.title(\"Number of Chunks by Category\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "    else:\n",
    "        logger.info(\"No categories found for plotting category distribution.\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "logger.info(\"--- Starting Markdown Chunking Process ---\")\n",
    "chunking_start_time = time.time()\n",
    "\n",
    "# Ensure the input directory exists\n",
    "if not Path(INPUT_MARKDOWN_DIR).exists():\n",
    "    logger.error(f\"Input directory '{INPUT_MARKDOWN_DIR}' does not exist.\")\n",
    "else:\n",
    "    files_processed_count, chunks_created_count = process_markdown_directory(\n",
    "        INPUT_MARKDOWN_DIR,\n",
    "        OUTPUT_CHUNKS_DIR,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    if chunks_created_count > 0:\n",
    "        # Analyze the chunk distribution from the aggregated file\n",
    "        aggregated_file = Path(OUTPUT_CHUNKS_DIR) / \"all_chunks_aggregated.json\"\n",
    "        chunk_stats_df = analyze_chunk_distribution(aggregated_file)\n",
    "        if chunk_stats_df is not None:\n",
    "            logger.info(\"\\nChunk statistics DataFrame head:\")\n",
    "            display(chunk_stats_df.head())\n",
    "\n",
    "chunking_end_time = time.time()\n",
    "elapsed_time = chunking_end_time - chunking_start_time\n",
    "logger.info(f\"--- Markdown Chunking Process Completed in {elapsed_time:.2f} seconds ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
